<!doctype html>
<html lang="en">
    <head>
        <title>How Far is Video Generation from World Model: A Physical Law Perspective</title>

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://phyworld.github.io/" />
        <!-- TODO: Modify it later -->
        <meta property="og:image" content="https://phyworld.github.io/static/img/preview.png" />
        <meta property="og:title" content="How Far is Video Generation from World Model: A Physical Law Perspective" />
        <meta property="og:description" content="We conduct a systematic study to investigate whether video generation is able to learn physical laws from videos, leveraging data and model scaling." />
        
        <!-- Twitter -->
        <meta name="twitter:url" content="https://phyworld.github.io/" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:image" content="https://phyworld.github.io/static/img/preview.png" />
        <meta name="twitter:title" content="How Far is Video Generation from World Model: A Physical Law Perspective" />
        <meta name="twitter:description" content="We conduct a systematic study to investigate whether video generation is able to learn physical laws from videos, leveraging data and model scaling." />

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>
            MathJax = {
                tex: {
                    inlineMath: [['$', '$']]
                }
            };
        </script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>
        <script src="./static/js/switch_videos.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>


        <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>

        <style>
            /* Hide controls on hover for the specific video with class 'no-controls' */
            .no-controls::-webkit-media-controls {
                display: none !important;
            }
            .no-controls:hover::-webkit-media-controls {
                display: none !important;
            }
            /* Applying the same for Firefox */
            .no-controls::-moz-media-controls {
                display: none !important;
            }
            .no-controls:hover::-moz-media-controls {
                display: none !important;
            }
            #twitter_container {
                column-width: 250px;
                column-gap: 20px;
            }
            .table-container {
                margin: 20px 0;
                overflow-x: auto;
            }
            
            .table-container table {
                border-collapse: collapse;
                width: 100%;
                font-size: 0.9em;
            }
            
            .table-container th, .table-container td {
                border: 1px solid #ddd;
                padding: 8px;
                text-align: center;
            }
            
            .table-container th {
                background-color: #f5f5f5;
                font-weight: bold;
            }
            
            .table-container tr:hover {
                background-color: #f5f5f5;
            }
            
            .table-container caption {
                margin-bottom: 10px;
                text-align: left;
                font-weight: bold;
            }

            .data-table {
                width: 100%;
                border-collapse: collapse;
                margin: 20px 0;
            }

            .data-table th,
            .data-table td {
                padding: 12px 15px;
                border: 1px solid #ddd;
                text-align: center;
            }

            .data-table th {
                background-color: #f5f5f5;
                font-weight: bold;
            }

            .data-table tr:hover {
                background-color: #f5f5f5;
            }
        </style>
        </head>
        <body>
            <div class="header-wrapper">
                <div class="header-container" id="header-container">
                    <div class="header-content">
                        <h1 style="margin-top: 0px; font-size: xxx-large;">ImageGen-CoT</h1>
                        <h2 style="margin-top: 0px; font-size: xx-large;">Enhancing Text-to-Image In-context Learning with Chain-of-Thought Reasoning</h2>
                        <p>
                            We propose a novel framework that generates a thought process (called ImageGen-CoT) to enhance the performance of unified MLLMs in T2I-ICL tasks. Our contributions can be summarized as follows:
                        </p>
                        <div class="contributions">
                            <div class="contribution-item">
                                <h3>🎯 Chain-of-Thought Prompting</h3>
                                <p>We propose a novel framework that incorporates a thought process called ImageGen-CoT prior to image generation in T2I-ICL tasks.</p>
                            </div>
                            <div class="contribution-item">
                                <h3>📊 FT w ImageGen-CoT dataset</h3>
                                <p>We construct high-quality ImageGen-CoT datasets for fine-tuning unified MLLMs through an automatic dataset construction pipeline.</p>
                            </div>
                            <div class="contribution-item">
                                <h3>⚡ Efficient Scaling Strategy</h3>
                                <p>We explore Best-of-N test-time scaling up paradigms and propose a hybrid scaling approach that first generates multiple ImageGen-CoT chains and then generates multiple image variations per chain.</p>
                            </div>
                        </div>
                        

    
                        <div class="button-container", style="text-align: center;">
                            <a href="https://arxiv.org/abs/[YourPaperID]" class="button paper-link" target="_blank">
                                <span class="icon is-small">
                                    <i class="ai ai-arxiv"></i>
                                </span>
                                arXiv
                            </a>
                            <a href="static/ImageGen_CoT.pdf" class="button paper-link" target="_blank">
                                <span class="icon is-small">
                                    <i class="fas fa-file-pdf"></i>
                                </span>
                                <span>PDF</span>
                            </a>
                            <a href="https://github.com/JiaqiLiao77/ImageGen-CoT" class="button" target="_blank">
                                <span class="icon is-small">
                                    <i class="fab fa-github"></i>
                                </span>
                                <span>Code</span>
                            </a>
                            <a href="#" class="button" target="_blank">
                                <span class="icon is-small">
                                    <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" style="height: 1em;">
                                </span>
                                <span>Data</span>
                            </a>                        
                        </div>


                </div>
                <div class="header-image">
                    <img draggable="false" src="static/image/ImageGen-CoT.webp" class="teaser-image" width="100%">
                </div>
            </div>
        </div>
    
    <d-article>

        <div class="byline">
            <div class="byline-container">
                <div class="is-size-5 publication-authors">
                    <div style="margin-bottom: px;">
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Jiaqi Liao<sup>†</sup><sup style="color:#6fbf73;">1</sup></a>,
                        </span>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Zhengyuan Yang<sup style="color:#6fbf73;">1</sup></a>,
                        </span>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Linjie Li<sup style="color:#6fbf73;">1</sup></a>,
                        </span>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Dianqi Li</a>,
                        </span>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Kevin Lin<sup style="color:#6fbf73;">1</sup></a>,
                        </span>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Yu Cheng<sup style="color:#ffac33;">2</sup></a>,
                        </span>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Lijuan Wang<sup style="color:#6fbf73;">1</sup><sup>✉</sup></a>
                        </span>
                    </div>
                </div>
        
                <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup style="color:#6fbf73;">1</sup>Microsoft,</span>
                    <span class="author-block"><sup style="color:#ffac33;">2</sup>The Chinese University of Hong Kong</span>
                </div>
        
                <div class="is-size-5 publication-authors">
                    <span class="author-note" style="color: black;">†Interns at Microsoft.</span>
                </div>
            </div>
        </div>
        
        

        <p class="text abstract" style="margin-top: 5%;">
            <span style="color: blue;"><strong>Background</strong></span><br>
            Human intelligence excels at learning novel concepts through contextual observation and adapting to new inputs. When presented with a series of interleaved text-image examples—such as "a leather-bound book," followed by "a leather apple"—and then asked to generate an image for the query "a box," humans intuitively infer the implicit pattern of "leather" and apply it to the new query, resulting in "a leather box." This reasoning ability to learn novel concepts from multimodal contexts underpins creative problem-solving. Existing unified Multimodal Large Language Models (unified MLLMs) have demonstrated remarkable capabilities in multimodal understanding and generation within a single model architecture. Given their ability to process and generate across modalities similar to human cognition, it is natural to investigate whether these models can exhibit reasoning capabilities comparable to those of humans. To evaluate this, we adopt the Text-to-Image In-Context Learning (T2I-ICL) task, which requires models to process interleaved text-image inputs and generate coherent outputs by learning from multimodal contexts (<a href="#figure-1">Figure 1</a>). Despite the impressive capabilities of unified MLLMs, our experiments reveal that they struggle to replicate this reasoning capability, often failing to grasp contextual relationships or preserve compositional consistency in T2I-ICL tasks.
        </p>

        <d-figure id="figure-1">
            <figure>
                <img data-zoomable="" draggable="false" src="static/image/teaser_00.jpg" alt="Explainable Analysis Figure" width="100%">
                <figcaption>
                    <strong>Figure 1:</strong> <strong>Comparisons between SEED-X and SEED-X FT w ImageGen-CoT dataset.</strong>
                </figcaption>
            </figure>
        </d-figure>
        

        <p class="text abstract" style="margin-top: 3%;">
            <span style="color: blue;"><strong>Contribution</strong></span><br>
            We propose a novel framework that generates a thought process (called ImageGen-CoT) to enhance the performance of unified MLLMs in T2I-ICL tasks. Our contributions can be summarized as follows:
            <ul class="text" style="margin-top: 0%; padding-left: 10%;">
                <li style="margin-bottom: 0;">1. <a href="#chain_of_thought"><strong>Chain-of-Thought Prompting</strong></a>: We propose a novel framework that incorporates a thought process called ImageGen-CoT prior to image generation in T2I-ICL tasks.</li>
                <li style="margin-bottom: 0;">2. <a href="#ft_dataset"><strong>FT w ImageGen-CoT dataset</strong></a>: We construct high-quality ImageGen-CoT datasets for fine-tuning unified MLLMs through an automatic dataset construction pipeline.</li>
                <li style="margin-bottom: 0;">3. <a href="#scaling_strategy"><strong>Efficient Scaling Strategy</strong></a>: We explore Best-of-N test-time scaling up paradigms and propose a hybrid scaling approach that first generates multiple ImageGen-CoT chains and then generates multiple image variations per chain.</li>
            </ul>
        </p>

        <p class="text abstract" style="margin-top: 3%;">
            <span style="color: blue;"><strong><a href="#validation">Validation</a></strong></span><br>
            Our method significantly improves SEED-X's performance through progressive enhancements: adding ImageGen-CoT, fine-tuning with the ImageGen-CoT dataset, and applying test-time scaling strategies. Specifically, as shown in <a href="#figure-2">Figure 2</a>, SEED-X FT with ImageGen-CoT improves by 89% and 114% on CoBSAT and DreamBench++. With scaling strategy, it further achieves 0.909 and 0.543 respectively.
        </p>

        <d-figure id="figure-2">
            <figure>
                <img data-zoomable="" draggable="false" src="static/image/teaser_result_new_00.jpg" alt="Explainable Analysis Figure" width="100%">
                <figcaption>
                    <strong>Figure 2:</strong> <strong>Performance comparison on CoBSAT and DreamBench++ benchmarks.</strong>
                </figcaption>
            </figure>
        </d-figure>

        <div id="law_discovery" class="section">
            <h1 class="text">Method</h1>

            <p class="text">
                In this section, we present our ImageGen-CoT framework in detail. First, we introduce the formulation of ImageGen-CoT (<a href="#Formulation_of_ImageGen-CoT"><strong>Formulation of ImageGen-CoT</strong></a>). Second, we describe our automated pipeline for collecting high-quality ImageGen-CoT datasets (<a href="#Dataset_Construction"><strong>Dataset Construction</strong></a>). Third, we explore various strategies to enhance model performance during inference, culminating in a novel hybrid scaling approach that addresses both contextual comprehension and generation challenges (<a href="#Scaling_Strategy"><strong>Scaling Strategy</strong></a>).
            </p>

            <d-figure id="figure-3">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/image/MSR_Main_paper_00.jpg" alt="Explainable Analysis Figure" width="100%">
                    <figcaption>
                        <strong>Figure 3:</strong> <strong>Main Pipeline. (a) Data Collection Pipeline (b) Training Pipeline (c) Test-Time Scaling</strong>
                    </figcaption>
                </figure>
            </d-figure>
            
            <h3 id="Formulation_of_ImageGen-CoT" class="text">Formulation of ImageGen-CoT</h3>

            <p class="text">
                As described above, T2I-ICL tasks require models to have a high level of comprehension. To enhance the model's capacity, we propose a new framework that generates a Chain-of-Thought, which we call ImageGen-CoT, before performing ImageGen. While we initially expected models to simultaneously output both ImageGen-CoT reasoning chains and corresponding images in a single forward pass.
            </p>

            <p class="text">
                However, during our practice, we observe that models frequently fail to generate images even when explicitly prompted to first generate ImageGen-CoT followed by image output. As illustrated in <a href="#figure-3">Figure 3</a>, to ensure reliable image generation, we develop a two-stage inference protocol. The first stage involves prompting the model to generate the ImageGen-CoT reasoning chain R. In the second stage, we combine the original input X with the generated ImageGen-CoT R, along with a mandatory image generation token ⟨image⟩, to guarantee the production of the target image I. This process can be formally expressed as:
            </p>

            <p class="text">
            \begin{equation}
            \begin{aligned}
            & Stage 1: R = \mathcal{M}(X \oplus instruction) \\
            & Stage 2: I = \mathcal{M}(X \oplus R \oplus \langle image \rangle)
            \end{aligned}
            \end{equation}
            </p>

            <p class="text">
                where $\mathcal{M}$ denotes the unified MLLMs, and $\oplus$ represents the concatenation operation.
            </p>

            <h3 id="Dataset_Construction" class="text">Dataset Construction</h3>

            <p class="text">
                To collect high-quality ImageGen-CoT datasets, we first establish an instruction pool by collecting instructions from existing training datasets in T2I-ICL tasks. Second, we propose an automatic dataset construction pipeline. In the initial stage, we let MLLM act as a <strong>Generator</strong> to generate N outputs, each consisting of an ImageGen-CoT and a prompt for the next image, which are then used by T2I-Model to generate N images. Then, MLLM acts as a <strong>Selector</strong> to select the best image from the N candidates. After that, if the selected image meets our quality threshold or reaches the maximum iteration limit, the pipeline terminates and outputs the corresponding ImageGen-CoT and image pair. Otherwise, we let MLLM act as a <strong>Critic</strong> to write a critique of the selected image, assessing how well it matches the T2I-ICL prompt. Finally, MLLM acts as a <strong>Refiner</strong> to refine the prompt based on the critique, and the process iterates until meeting the termination.
            </p>

            <h3 id="Scaling_Strategy" class="text">Scaling Strategy</h3>

            <p class="text">
                Though fine-tuning with the ImageGen-CoT dataset significantly improves model performance in T2I-ICL tasks, substantial room for improvement remains. Inspired by test-time scaling methods in NLP, we explore whether increasing computational investment during inference can further enhance performance. We first investigate a conventional paradigm: using SEED-X as the base model, generating multiple images by varying the seed value, and outputs are filtered via a ground-truth verifier aligned with the Pass@N metric. However, we observe that even with N=16, this approach underperforms compared to SEED-X fine-tuned with ImageGen-CoT Dataset.
            </p>

            <p class="text">
                This observation motivates our exploration of test-time scaling in the context of ImageGen-CoT, which we approach through three distinct strategies:
            </p>

            <ol class="text">
                <li><strong>Single-Chain Scaling:</strong> This approach generates one ImageGen-CoT chain and produces multiple image variants by varying the seed values.</li>
                
                <li><strong>Multi-Chain Scaling:</strong> Similar to NLP's "Best-of-N" sampling, we generate multiple ImageGen-CoT chains through high-temperature LLM decoding. Each chain produces a unique image, potentially capturing different aspects of the contextual requirements.</li>
                
                <li><strong>Hybrid Scaling:</strong> Regarding the dual challenges of contextual comprehension and generation in T2I-ICL tasks, we propose a hybrid approach that combines the strengths of both strategies. As illustrated in <a href="#figure-3">Figure 3</a>, this method first generates multiple ImageGen-CoT chains and then creates multiple image variations for each chain.</li>
            </ol>

            
        </div>

        <div id="law_discovery" class="section">
            <h1 class="text">Validation</h1>

            <p class="text">
                In this section, we seek to answer the following questions: a) How much the ImageGen-CoT improves model's performance (via prompting)? b) To what extent does the performance of the model improve after fine-tuning with the ImageGen-CoT dataset? c) Can we invest more time in inference time to improve the performance? Finally, to better demonstrate the effectiveness of our method, we present visible comparison results.
            </p>

            <h3 id="prompting_effect" class="text">Effect of ImageGen-CoT Prompting</h3>

            <p class="text">
                As shown in <strong><a href="#table-1">Table 1</a></strong> and <strong><a href="#table-2">Table 2</a></strong>, integrating ImageGen-CoT through prompting yields consistent improvements across benchmarks. On CoBSAT, SEED-LLaMA's average score improves from 0.254 to 0.283 (+11.4% relative gain), while SEED-X shows a more substantial improvement from 0.349 to 0.439 (+25.8%). The trend persists on Dreambench++, where SEED-X achieves a 84.6% relative improvement (0.188 → 0.347) compared to its baseline. These results highlight the effectiveness of incorporating ImageGen-CoT in enhancing model performance.
            </p>

            <div id="tab:model_ensemble" style="display: flex; flex-direction: column; align-items: center;">
                <div class="table-container">
                    <table class="data-table">
                        <caption style="caption-side: top; text-align: left; margin-bottom: 10px;">
                            <strong>Table 1: Main results on CoBSAT benchmark.</strong>
                        </caption>
                        <thead>
                            <tr>
                                <th rowspan="2">Method</th>
                                <th colspan="5">Object-Inference Task</th>
                                <th colspan="5">Attribute-Inference Task</th>
                                <th rowspan="2">Avg.↑</th>
                            </tr>
                            <tr>
                                <th>Color-I</th>
                                <th>Bkg-I</th>
                                <th>Style-I</th>
                                <th>Action-I</th>
                                <th>Texture-I</th>
                                <th>Color-II</th>
                                <th>Bkg-II</th>
                                <th>Style-II</th>
                                <th>Action-II</th>
                                <th>Texture-II</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>SEED-LLaMA</td>
                                <td>.616</td>
                                <td>.216</td>
                                <td>.272</td>
                                <td>.592</td>
                                <td>.112</td>
                                <td>.088</td>
                                <td>.168</td>
                                <td>.192</td>
                                <td>.220</td>
                                <td>.056</td>
                                <td>.254</td>
                            </tr>
                            <tr>
                                <td>+ ImageGen-CoT (via Prompt)</td>
                                <td>.700</td>
                                <td>.276</td>
                                <td>.300</td>
                                <td>.408</td>
                                <td>.084</td>
                                <td>.176</td>
                                <td>.292</td>
                                <td>.272</td>
                                <td>.192</td>
                                <td>.132</td>
                                <td>.283</td>
                            </tr>
                            <tr>
                                <td>+ FT w/ GT Image</td>
                                <td>.632</td>
                                <td>.272</td>
                                <td>.352</td>
                                <td>.540</td>
                                <td>.128</td>
                                <td>.164</td>
                                <td>.200</td>
                                <td>.256</td>
                                <td>.172</td>
                                <td>.112</td>
                                <td>.283</td>
                            </tr>
                            <tr style="background-color: #e6f3ff;">
                                <td>+ FT w/ ImageGen-CoT Dataset</td>
                                <td>.620</td>
                                <td>.368</td>
                                <td>.384</td>
                                <td>.424</td>
                                <td>.060</td>
                                <td>.192</td>
                                <td>.288</td>
                                <td>.208</td>
                                <td>.216</td>
                                <td>.148</td>
                                <td>.291 <span style="color: red;">(↑14.6%)</span></td>
                            </tr>
                            <tr>
                                <td>SEED-X</td>
                                <td>.796</td>
                                <td>.412</td>
                                <td>.316</td>
                                <td>.596</td>
                                <td>.240</td>
                                <td>.176</td>
                                <td>.344</td>
                                <td>.260</td>
                                <td>.252</td>
                                <td>.104</td>
                                <td>.349</td>
                            </tr>
                            <tr>
                                <td>+ ImageGen-CoT (via Prompt)</td>
                                <td>.724</td>
                                <td>.440</td>
                                <td>.660</td>
                                <td>.784</td>
                                <td>.216</td>
                                <td>.312</td>
                                <td>.472</td>
                                <td>.228</td>
                                <td>.320</td>
                                <td>.240</td>
                                <td>.439</td>
                            </tr>
                            <tr>
                                <td>+ FT w/ GT Image</td>
                                <td>.936</td>
                                <td>.712</td>
                                <td>.896</td>
                                <td>.860</td>
                                <td>.468</td>
                                <td>.280</td>
                                <td>.324</td>
                                <td>.388</td>
                                <td>.636</td>
                                <td>.424</td>
                                <td>.592</td>
                            </tr>
                            <tr style="background-color: #e6f3ff;">
                                <td>+ FT w/ ImageGen-CoT Dataset</td>
                                <td>.884</td>
                                <td>.692</td>
                                <td>.928</td>
                                <td>.936</td>
                                <td>.420</td>
                                <td>.504</td>
                                <td>.612</td>
                                <td>.660</td>
                                <td>.524</td>
                                <td>.424</td>
                                <td>.658 <span style="color: red;">(↑88.5%)</span></td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <h3 id="finetuning_effect" class="text">Impact of Fine-tuning with ImageGen-CoT Dataset</h3>

            <p class="text">
                As shown in <strong><a href="#table-1">Table 1</a></strong>, SEED-LLaMA and SEED-X fine-tuned with ImageGen-CoT Dataset achieve improvements of +2.8% (0.283 → 0.291) and +49.9% (0.439 → 0.658), compared to generating ImageGen-CoT via prompting, respectively. What's more, they even outperform themselves fine-tuned with GT Images by +2.8% (0.283 → 0.291) and +11.1% (0.592 → 0.658). Additionally, on the Dreambench++ benchmark, SEED-LLaMA fine-tuned with ImageGen-CoT Dataset shows an improvement of +29.5% (0.078 → 0.101) in CP·PF score, while SEED-X achieves a +16.1% gain (0.347 → 0.403). These strong results on COBSAT and Dreambench++ underscore the effectiveness and generalizability of our collected ImageGen-CoT dataset in enhancing model reasoning and understanding abilities.
            </p>

            <div id="tab:model_ensemble" style="display: flex; flex-direction: column; align-items: center; margin-bottom: 30px;">
                <div class="table-container">
                    <table class="data-table">
                        <caption style="caption-side: top; text-align: left; margin-bottom: 10px;">
                            <strong>Table 2: Evaluation results on Dreambench++ benchmark.</strong>
                        </caption>
                        <thead>
                            <tr>
                                <th rowspan="2">Method</th>
                                <th colspan="5">Concept Preservation</th>
                                <th colspan="4">Prompt Following</th>
                                <th rowspan="2">CP·PF↑</th>
                            </tr>
                            <tr>
                                <th>Animal</th>
                                <th>Human</th>
                                <th>Object</th>
                                <th>Style</th>
                                <th>Overall</th>
                                <th>Photorealistic</th>
                                <th>Style</th>
                                <th>Imaginative</th>
                                <th>Overall</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>SEED-LLaMA</td>
                                <td>.436</td>
                                <td>.315</td>
                                <td>.288</td>
                                <td>.381</td>
                                <td>.358</td>
                                <td>.306</td>
                                <td>.202</td>
                                <td>.154</td>
                                <td>.218</td>
                                <td>.078</td>
                            </tr>
                            <tr>
                                <td>+ ImageGen-CoT (via Prompt)</td>
                                <td>.390</td>
                                <td>.241</td>
                                <td>.262</td>
                                <td>.346</td>
                                <td>.317</td>
                                <td>.291</td>
                                <td>.211</td>
                                <td>.170</td>
                                <td>.222</td>
                                <td>.078</td>
                            </tr>
                            <tr style="background-color: #e6f3ff;">
                                <td>+ FT w/ ImageGen-CoT Dataset</td>
                                <td>.399</td>
                                <td>.290</td>
                                <td>.271</td>
                                <td>.318</td>
                                <td>.325</td>
                                <td>.348</td>
                                <td>.355</td>
                                <td>.210</td>
                                <td>.310</td>
                                <td>.101 <span style="color: red;">(↑29.5%)</span></td>
                            </tr>
                            <tr>
                                <td>SEED-X</td>
                                <td>.647</td>
                                <td>.420</td>
                                <td>.526</td>
                                <td>.571</td>
                                <td>.559</td>
                                <td>.346</td>
                                <td>.342</td>
                                <td>.303</td>
                                <td>.337</td>
                                <td>.188</td>
                            </tr>
                            <tr>
                                <td>+ ImageGen-CoT (via Prompt)</td>
                                <td>.547</td>
                                <td>.293</td>
                                <td>.369</td>
                                <td>.424</td>
                                <td>.427</td>
                                <td>.862</td>
                                <td>.775</td>
                                <td>.737</td>
                                <td>.817</td>
                                <td>.347</td>
                            </tr>
                            <tr style="background-color: #e6f3ff;">
                                <td>+ FT w/ ImageGen-CoT Dataset</td>
                                <td>.549</td>
                                <td>.410</td>
                                <td>.403</td>
                                <td>.432</td>
                                <td>.458</td>
                                <td>.922</td>
                                <td>.851</td>
                                <td>.846</td>
                                <td>.881</td>
                                <td>.403 <span style="color: red;">(↑114.4%)</span></td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <h3 id="scaling_effect" class="text">Benefits of Test-time Scaling Strategy</h3>

            <p class="text">
                As shown in <strong><a href="#figure-4">Figure 4</a></strong>, our experiments reveal three key insights. First, the Vanilla SEED-X@16 baseline (0.67 on CobSAT, 0.312 on Dreambench++) underperforms even the simplest scaling strategies (e.g., 0.747 on CobSAT@2), highlighting the necessity of ImageGen-CoT integration. Second, Multi-Chain Scaling matches Single-Chain Scaling in performance, proving that generating diverse reasoning paths is as effective as varying outputs from a single chain. Finally, Hybrid Scaling consistently achieves the highest scores across benchmarks. At N=16, Hybrid Scaling improves CobSAT performance to 0.909 (1.9% over Single-Chain) and Dreambench++ to 0.543 (0.8% higher than Single-Chain). The integration of ImageGen-CoT enables effective bidirectional scaling across both comprehension and generation dimensions. This dual-axis scalability suggests new pathways for optimizing MLLM performance in complex multimodal tasks.
            </p>

            <d-figure id="figure-4">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/image/Scaleup_exp_00.jpg" alt="Explainable Analysis Figure" width="100%">
                    <figcaption>
                        <strong>Figure 4:</strong> <strong>Test-time scaling strategies comparison.</strong> We conducted a comprehensive evaluation of three distinct scaling strategies: Multi-Chain Scaling, Single-Chain Scaling, and Hybrid Scaling.
                    </figcaption>
                </figure>
            </d-figure>
            
            <h3 id="qualitative_results" class="text">Qualitative Results</h3>

            <p class="text">
                As shown in the top of <strong><a href="#figure-5">Figure 5</a></strong>, baseline SEED-X (b) generates a basic book shape but misses the implicit "lace" attribute. With ImageGen-CoT prompting (c), the model's weak comprehension leads to poor ImageGen-CoT quality and even degraded generation quality. After fine-tuning with ImageGen-CoT dataset (d), with help of ImageGen-CoT, the model first successfully infers the shared attribute "lace" in CoT text and then generates the correct image - a book made of lace. Similarly, as shown in the bottom of <strong><a href="#figure-5">Figure 5</a></strong>, baseline SEED-X (b) only generates a simple egg with an open mouth, ignoring key requirements like "on stone", "in garden", and similar expression (sad with upturned closed mouth). With ImageGen-CoT prompting (c), while the egg is placed on stone, it lacks both the required facial expression and garden environment. After fine-tuning (d), the model successfully understands all task requirements and generates a complete scene with an egg properly placed on a stone in a garden setting, maintaining similar facial features to the input. These qualitative results visually demonstrate the effectiveness of ImageGen-CoT and its corresponding dataset in enhancing model comprehension and generation capability, particularly in handling complex tasks that require attention to detail and scene understanding.
            </p>

            <d-figure id="figure-5">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/image/visible_results_msr_00.jpg" alt="Explainable Analysis Figure" width="100%">
                    <figcaption>
                        <strong>Figure 5:</strong> <strong>Qualitative Results.</strong> Comparison of generated images from different model variants.
                    </figcaption>
                </figure>
            </d-figure>

        </div>

        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>   
        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>
        <script src="./static/js/nav-bar.js"></script>
        <script>
            function toggleContent() {
                const content = document.getElementById("twitter_container");
                // 切换display属性
                if (content.style.display === "none") {
                    content.style.display = "block"; // 展开
                } else {
                    content.style.display = "none"; // 折叠
                }
            }
        </script>
    </body>
</html>
